# Подробная документация по поиску торговых уровней на графиках

## Введение

**Торговые уровни поддержки и сопротивления** – ключевые понятия технического анализа. Под **уровнем поддержки** понимается ценовая область, где нисходящее движение замедляется или останавливается из-за возросшего спроса – цена **не склонна падать ниже** этой отметки. В свою очередь, **уровень сопротивления** – ценовая область, где восходящее движение замедляется или останавливается из-за увеличения предложения – цена **с трудом поднимается выше** этой отметки

[investopedia.com](https://www.investopedia.com/trading/support-and-resistance-basics/#:~:text=Technical%20analysis%20acknowledges%20that%20all,at%20which%20buyers%20back%20off)

. На графике поддержки часто выступают своеобразным "полом" (цена удерживается сверху), а сопротивления – "потолком" (цена ограничена снизу). Выявление этих уровней помогает трейдерам находить оптимальные точки входа и выхода из позиций, так как именно около них повышается вероятность разворота или приостановки текущего тренда

[investopedia.com](https://www.investopedia.com/trading/support-and-resistance-basics/#:~:text=,identified%20on%20charts%20using%20trendlines)

.

Однако определение уровней **вручную** на глаз может быть субъективным и требовать опыта. В алгоритмическом трейдинге важно **автоматически** вычислять эти уровни по историческим данным. В данном документе представлена подробная техническая инструкция по реализации алгоритма поиска торговых уровней на графиках с использованием Python и популярных библиотек: **Pandas** и **NumPy** для обработки данных, **Pybit** для получения котировок с биржи Bybit через API, а также **Scikit-learn** и **PyTorch** для реализации методов машинного обучения. Визуализация результатов будет выполнена средствами **Matplotlib** и **Plotly** (в том числе интеграция в веб-приложение на **Dash**).

Мы рассмотрим два подхода к поиску уровней:

- **Алгоритмический метод** – строгий алгоритм на основе правил (поиск локальных экстремумов и группировка их по ценам).
- **AI/ML метод** – использование методов машинного обучения (кластеризация и простые нейросети) для обнаружения уровней.

Кроме того, продемонстрируем, как применять индикатор **Average True Range (ATR)** для динамической настройки допустимого отклонения (*«люфта»*) около уровней в зависимости от волатильности рынка. Все этапы – от получения и подготовки данных до визуализации уровней – будут подробно документированы с примерами кода и комментариями.

## Получение и подготовка исходных данных

Прежде чем искать уровни, необходимо получить исторические ценовые данные. Мы будем использовать биржу **Bybit** (криптовалютный рынок) в качестве источника данных. Исторические данные можно получить двумя способами:

1. **Через API биржи Bybit** – с помощью официальной библиотеки **Pybit** (Python SDK для Bybit).
2. **Из CSV-файлов** – например, выгруженных ранее данных.

В этом разделе опишем оба способа.

### Получение данных через API Bybit (Pybit)

Библиотека Pybit предоставляет удобный интерфейс для запросов к REST API Bybit. Чтобы начать, нужно установить сам Pybit (`pip install pybit`) и получить API-ключ и секрет для Bybit (если требуется авторизация; для публичных рыночных данных часто не нужен ключ). Ниже пример кода для получения исторических свечей (OHLC – Open, High, Low, Close) с Bybit:

```python
python
КопироватьРедактировать
from pybit.unified_trading import HTTP
import pandas as pd

# Инициализируем сессию REST API (testnet=False означает боевой счет)
session = HTTP(testnet=False, api_key="ВАШ_API_KEY", api_secret="ВАШ_SECRET_KEY")

# Функция для получения исторических свечей
def get_historical_klines(symbol: str, interval: int, start_time: str, end_time: str):
    """
    symbol: торговый инструмент, например "BTCUSDT".
    interval: таймфрейм свечи в минутах (например, 1 -> 1 минута, 60 -> 1 час и т.д.).
    start_time, end_time: время начала и окончания периода в формате "YYYY-MM-DD HH:MM:SS".
    """
    from datetime import datetime
    import math, time

    # Преобразуем время в миллисекунды UNIX
    start_ts = int(datetime.strptime(start_time, "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
    end_ts   = int(datetime.strptime(end_time, "%Y-%m-%d %H:%M:%S").timestamp() * 1000)

    data_frames = []  # для накопления результатов
    limit = 1000  # максимальное количество баров за один запрос (ограничение API)
    while start_ts < end_ts:
        # Запрос исторических свечей
        response = session.get_kline(symbol=symbol, interval=str(interval), start=start_ts, end=end_ts, limit=limit, category="linear")
        result = response.get('result')
        if result is None or result.get('list') is None:
            break  # если данных больше нет
        candles = result['list']
        if len(candles) == 0:
            break
        # Преобразуем в DataFrame
        df = pd.DataFrame(candles, columns=["timestamp", "open", "high", "low", "close", "volume"])
        data_frames.append(df)
        # Время последнего полученного бара
        last_ts = int(candles[-1][0])
        # Сдвигаем старт на следующий после последнего полученного
        start_ts = last_ts + 1
        # Задержка, чтобы не превысить лимиты API
        time.sleep(0.1)
    if not data_frames:
        return None
    data = pd.concat(data_frames, ignore_index=True)
    # Конвертируем время из миллисекунд в datetime и устанавливаем в индекс
    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')
    data = data.set_index('timestamp')
    # Приводим столбцы к числовым типам
    data = data.astype(float)
    return data

# Пример вызова функции для получения часовых свечей BTCUSDT за определенный период
df = get_historical_klines("BTCUSDT", interval=60, start_time="2023-01-01 00:00:00", end_time="2023-02-01 00:00:00")
print(df.head())

```

*Комментарии к коду:*

- Мы используем `session.get_kline` метода HTTP API v5, передавая необходимые параметры: торговый символ, интервал свечей (в минутах), время начала (`start`) и окончания (`end`) периода в миллисекундах, и `limit` – сколько свечей вернуть за один запрос (максимум 1000).
- Так как API выдает ограниченное число баров за запрос, реализован цикл, который смещает окно запроса, чтобы последовательно выгрузить все данные за нужный диапазон дат. После каждого запроса `start_ts` сдвигается на последнюю полученную метку времени + 1 мс, пока не дойдем до `end_ts`.
- Собранные части данных объединяются в один DataFrame `pandas`. В итоге получаем таблицу с колонками: время (`timestamp`), цены открытия (`open`), максимума (`high`), минимума (`low`), закрытия (`close`) и объем (`volume`).
- Временная метка превращается в datetime и ставится индексом DataFrame для удобства временных операций.
- **Примечание:** Для больших объемов данных нужно учитывать ограничения по частоте запросов и объему возвращаемых данных. В нашем примере предусмотрена пауза `time.sleep(0.1)` для избежания rate limit. Кроме того, Bybit API может ограничивать выдачу истории (например, 200 свечей за раз на старой версии API), поэтому приходится делить запрос на несколько частей.
    
    [quantnomad.com](https://quantnomad.com/getting-historical-bars-from-bybit-api-with-python/#:~:text=Image%20%20123)
    

Альтернативно, можно воспользоваться публичными REST API без аутентификации (например, эндпоинт `/v2/public/kline/list` у Bybit

[quantnomad.com](https://quantnomad.com/getting-historical-bars-from-bybit-api-with-python/#:~:text=def%20get_bybit_bars)

), используя модуль

```
requests
```

вместо Pybit. Однако Pybit упрощает работу, инкапсулируя детали запроса.

### Загрузка данных из CSV

Если данные уже сохранены, например, в CSV-файле, их можно загрузить с помощью Pandas. Предположим, у нас есть CSV-файл `BTCUSDT_2023.csv` с колонками `timestamp, open, high, low, close, volume`. Вот как его прочитать:

```python
python
КопироватьРедактировать
import pandas as pd

df = pd.read_csv("BTCUSDT_2023.csv")
# Преобразуем колонку времени из UNIX-метки или строки в datetime, если необходимо
df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')  # если время в секундах UNIX
# Устанавливаем временную метку в качестве индекса
df.set_index('timestamp', inplace=True)
# Убеждаемся, что данные отсортированы по времени
df.sort_index(inplace=True)
print(df.info())
print(df.head(3))

```

*Комментарии:*

- Здесь мы используем `pd.read_csv` для чтения файла. Затем предполагается, что метка времени либо сразу распознается (если CSV хранит ISO-даты), либо ее нужно конвертировать. В данном примере показан случай, когда время хранится как UNIX timestamp в секундах (`unit='s'` в `pd.to_datetime`).
- Установка индекса времени (`set_index`) позволяет легко резамплировать данные, выбирать по дате и т.д.
- Команда `sort_index` сортирует строки по возрастанию времени (на случай, если данные в CSV неупорядочены).
- Метод `df.info()` позволяет убедиться в типах данных (цены должны быть числовыми типа float). При необходимости можно привести типы: `df = df.astype(float)` для цен и объема.

После загрузки данных (с API или CSV) у нас должен получиться Pandas DataFrame c колонками `open, high, low, close, volume` и индексом – временной меткой каждого бара. Теперь можно переходить к обработке данных и расчету дополнительных индикаторов.

## Обработка данных: Pandas и NumPy

На этом этапе мы предположим, что DataFrame `df` уже содержит ценовые ряды. Необходимо убедиться, что данные корректны и, при необходимости, выполнить предобработку:

- **Заполнение пропусков:** Если в данных есть пропущенные временные интервалы (например, биржа не работала или данные недоступны), можно либо заполнить их (`ffill`/`bfill` последними известными значениями), либо явно учесть, чтобы алгоритм не считал большой разрыв за уровень. В крипто-рынке торги идут 24/7, пропусков по времени обычно нет, но на традиционных биржах будут перерывы на ночь и выходные.
- **Сглаживание (опционально):** Алгоритм поиска уровней иногда улучшается при сглаживании цен (например, скользящим средним) перед поиском экстремумов. Это поможет отфильтровать шум. Однако чрезмерное сглаживание может пропустить настоящие краткосрочные уровни. Часто применяют небольшое сглаживание или используют специальный индикатор ZigZag для выявления значимых поворотов цен.
    
    [stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=In%20practical%20terms%20this%20could,that%20the%20ZigZag%20indicator%20uses)
    
- **Resampling (при необходимости):** Если исходные данные на низком таймфрейме (скажем, минутки) и нужно определить уровни на более высоком таймфрейме (например, дневные уровни), удобнее агрегировать данные. Pandas позволяет делать ресемплирование, например `df.resample('1D').agg({'open':'first','high':'max','low':'min','close':'last','volume':'sum'})` для перехода к дневным свечам.

В нашем алгоритме в целом можно работать с тем таймфреймом, на котором хотим искать уровни (час, день и т.д.), без дополнительного ресемплинга.

Чтобы эффективно использовать Python, будем применять возможности **NumPy** и **Pandas** для расчётов:

- Будем получать значения цен как массивы NumPy или Pandas Series для удобства вычислений разниц, максимумов, минимумов и т.д.
- При поиске экстремумов можно сравнивать значения соседей с помощью сдвигов (`Series.shift` в Pandas или индексация массива NumPy).

**Пример предобработки:** Рассмотрим, что данные находятся в `df`. Убедимся, что индекс отсортирован, и вычислим, например, скользящую среднюю (для сглаживания) и индикатор ATR, который понадобятся далее:

```python
python
КопироватьРедактировать
# Убедимся, что данные отсортированы по времени
df.sort_index(inplace=True)

# (Опционально) добавим сглаженную цену закрытия (скользящее среднее за N периодов) для подавления шума
N = 5  # окно сглаживания
df['close_smooth'] = df['close'].rolling(window=N, min_periods=1).mean()

# Рассчитаем индикатор ATR (Average True Range) на периоде 14 (например)
period = 14
high = df['high']
low = df['low']
close = df['close']
prev_close = close.shift(1)

# Вычисляем True Range для каждого бара:
tr1 = high - low
tr2 = (high - prev_close).abs()
tr3 = (low - prev_close).abs()
df['TR'] = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

# ATR = скользящее среднее True Range за заданный period
df['ATR'] = df['TR'].rolling(window=period).mean()

```

*Пояснения:*

- `close_smooth` – скользящее среднее цены закрытия за окно `N`. Это упрощенное сглаживание. В более сложных случаях может использоваться фильтр или индикатор ZigZag для отсечения незначительных колебаний.
- **ATR (Average True Range)** – индикатор волатильности. Вычисляется как среднее значение показателя True Range за последние *period* баров. True Range для каждого бара определяется как наибольшее из: `high - low`, `|high - previous_close|`, `|low - previous_close|`. Таким образом, ATR учитывает разрывы (гэпы) между барами, беря максимальный размах. В коде выше мы вычисляем `TR` как максимальную из трех величин для каждой строки, а потом скользящее среднее `rolling(window).mean()` для получения ATR.
    
    [investopedia.com](https://www.investopedia.com/terms/a/atr.asp#:~:text=The%20true%20range%20indicator%20is,days%2C%20of%20the%20true%20ranges)
    
    [investopedia.com](https://www.investopedia.com/terms/a/atr.asp#:~:text=high%20less%20the%20current%20low%3B,days%2C%20of%20the%20true%20ranges)
    
- ATR обычно берется на период 14 дней по умолчанию (если дневные данные), но период можно изменять в зависимости от задачи и таймфрейма.
    
    [investopedia.com](https://www.investopedia.com/terms/a/atr.asp#:~:text=high%20less%20the%20current%20low%3B,days%2C%20of%20the%20true%20ranges)
    

После выполнения этих шагов DataFrame обогащается колонками `close_smooth`, `TR` и `ATR`. В дальнейшем ATR будет использоваться для адаптации алгоритма под волатильность.

## Логика поиска торговых уровней

Прежде чем переходить к реализации, опишем общую **логику определения уровней поддержки/сопротивления**:

1. **Поиск локальных экстремумов (точек разворота)**: Эти точки – кандидаты на уровни. Идея в том, что если цена несколько раз разворачивалась (из падения в рост или из роста в падение) около одной и той же ценовой отметки, то эта отметка является важным уровнем. Поэтому сначала находим **локальные минимумы** цен (возможные поддержки) и **локальные максимумы** (возможные сопротивления).
    
    [stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=Once%20you%20have%20local%20maxima,turning%20points%2C%20indicating%20possible%20support%2Fresistance)
    
2. **Фильтрация экстремумов по значимости**: Не каждый локальный экстремум значим. Можно вводить критерии – например, минимальное расстояние (по цене) до соседних экстремумов или минимальный перепад. Здесь пригодится **ATR**: на волатильном рынке мелкие колебания не столь важны, можно игнорировать экстремумы, которые отличаются от соседних цен меньше, чем некоторый процент от ATR.
3. **Группировка экстремумов в уровни**: Часто уровень – это не точная цена, а зона. Различные экстремумы, близкие по цене, скорее всего представляют один и тот же уровень (с погрешностью). Поэтому группируем найденные экстремальные точки, которые находятся «рядом» по цене (в пределах некоторого люфта/толеранса), и считаем их одним уровнем. Границы такой «зоны» можно определить по самым высоким и низким точкам группы, либо взять среднюю цену кластера точек.
    
    [stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=Once%20you%20have%20local%20maxima,turning%20points%2C%20indicating%20possible%20support%2Fresistance)
    
4. **Определение итоговых уровней поддержки и сопротивления**: После группировки получаем ряд ценовых уровней. Их можно разделить на уровни поддержки и сопротивления (например, исходя из того, на основе каких экстремумов – минимумов или максимумов – они образовались). Можно также ранжировать уровни по значимости – например, по количеству экстремумов в группе: уровень, где цена разворачивалась 3 раза, считается сильнее уровня с 1-2 касаниями.
    
    [stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=a%20fixed%20constant%20then%20you,turning%20points%2C%20indicating%20possible%20support%2Fresistance)
    
    [stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=You%20could%20then%20rank%20your,points%20at%20%2420%20for%20instance)
    

Важно понимать, что автоматический алгоритм не всегда идеально совпадет с интуицией трейдера. Он может найти слишком много уровней или пропустить некоторые. Поэтому параметры алгоритма (например, окно поиска экстремума, порог чувствительности, множитель ATR) нужно подбирать с учетом инструмента и таймфрейма.

Далее перейдем к реализации двух подходов поиска уровней: **алгоритмического** (правила и циклы) и **на основе машинного обучения**.

## Алгоритмический метод поиска уровней

Алгоритмический подход основан на прямом применении логики, описанной выше. Шаги будем выполнять последовательно, иллюстрируя кодом.

### 1. Поиск локальных максимумов и минимумов

Найдём все локальные минимумы и максимумы ценового ряда. Для этого можно использовать цену **Low** для минимумов и **High** для максимумов каждого бара (так как именно экстремумы дня часто выступают уровнями, а не обязательно цены закрытия). Будем считать точку локальным минимумом, если ее цена **LOW меньше**, чем у соседних точек (предыдущей и следующей), и локальным максимумом, если ее **HIGH больше**, чем у соседних. Для повышения надежности можно проверять не только ближайших соседей, но и несколько точек вокруг (например, ±2 бара), однако простейший вариант – сравнение с непосредственными соседями

[stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=In%20practical%20terms%20this%20could,that%20the%20ZigZag%20indicator%20uses)

.

**Пример реализации:**

```python
python
КопироватьРедактировать
# Исходные ряды цен для экстремумов
low_series = df['low']
high_series = df['high']

local_min_points = []  # список кортежей (время, цена) для локальных минимумов
local_max_points = []  # список (время, цена) для локальных максимумов

# Проходим по ряду цен, исключая первый и последний индекс (у них нет двух соседей)
for i in range(1, len(df) - 1):
    if low_series.iloc[i] < low_series.iloc[i-1] and low_series.iloc[i] < low_series.iloc[i+1]:
        # Найден локальный минимум
        local_min_points.append((df.index[i], low_series.iloc[i]))
    if high_series.iloc[i] > high_series.iloc[i-1] and high_series.iloc[i] > high_series.iloc[i+1]:
        # Найден локальный максимум
        local_max_points.append((df.index[i], high_series.iloc[i]))

print(f"Найдено локальных минимумов: {len(local_min_points)}, локальных максимумов: {len(local_max_points)}")

```

*Комментарии:*

- Мы перебираем индексы от 1 до `len(df)-2` и сравниваем текущий `low` с предыдущим и следующим – если он меньше обоих, то фиксируем как минимум.
- Аналогично для `high`.
- Можно использовать `df['low'].shift(1)` и `df['low'].shift(-1)` для сравнения без явного цикла (то есть `df['low'] < df['low'].shift(1)` & `df['low'] < df['low'].shift(-1)`, однако надо быть осторожным с границами массива и NaN).
- Полученные списки `local_min_points` и `local_max_points` содержат потенциальные **точки разворота**. Их довольно много, особенно на шумном графике, поэтому следующим шагом мы отфильтруем/сгруппируем их.

Дополнительные улучшения:

- **Учёт процента или ATR при определении экстремума:** Можно потребовать, чтобы разница между текущей ценой и соседями превышала, скажем, определенный процент или долю от ATR, чтобы отсеять “плоские” места. Например, добавив условие: `low[i-1] - low[i] > k*ATR[i]` и `low[i+1] - low[i] > k*ATR[i]` для минимума, где k – некоторый коэффициент (например, 0.1). Это исключит случаи, когда минимум лишь немного ниже соседних точек.
- **Сглаживание:** Как упоминалось, можно применять `close_smooth` вместо `close` или `low` для определения экстремумов, чтобы игнорировать мелкие колебания. Например, использовать сглаженную серию и находить экстремумы на ней, а потом уже смотреть исходные цены.

### 2. Группировка и объединение близких по цене экстремумов (кластеризация уровней)

На предыдущем шаге мы получили, возможно, десятки точек. Теперь наша задача – определить, какие из них указывают на один и тот же уровень. Например, если в разные дни были локальные минимумы по ценам 1.100 и 1.105, то, вероятно, это один уровень поддержки около ~1.10, учитывая, что разница всего 0.005 (0.5%).

Мы введем понятие **толеранса по цене (люфта)**: если две точки находятся на расстоянии менее определенного порога, будем считать их принадлежащими к одному кластеру (уровню). Этот порог можно **динамически выбрать на основе ATR** – например, возьмем порог = X% от среднего ATR за период, или X * ATR текущего периода. Идея в том, что на более волатильном рынке (высокий ATR) уровень имеет смысл шире (ценовой диапазон, зона), а на спокойном рынке – уже

[tradingview.com](https://www.tradingview.com/script/oY7rfzuc-ATR-Based-Support-and-Resistance-Zones-UAlgo/#:~:text=ATR%20Multiplier%3A%20The%20ATR%20multiplier,trading%20preferences%20and%20market%20conditions)

.

**Пример:** возьмем `tolerance = 0.5 * ATR_mean`, то есть полови́на среднего ATR за рассматриваемый период. Если средний ATR, скажем, 10 долларов (на дневках, например), то tolerance = 5 долларов – экстремумы, отличающиеся менее чем на $5, будут сгруппированы.

Реализуем группировку:

1. Разделим точки на **кандидаты поддержек** (local_min_points) и **кандидаты сопротивлений** (local_max_points), чтобы уровни поддержки и сопротивления обрабатывать раздельно. (Впрочем, их можно и вместе сгруппировать по цене, но удобнее раздельно, так как, например, на одной цене может быть и минимум и максимум в разное время – если уровень «сменил роль»).
2. Для каждой группы отсортируем точки по цене.
3. Пройдем по отсортированному списку, объединяя точки в кластер, пока расстояние между соседними точками в списке не превышает `tolerance`. Когда расстояние стало больше – начинаем новый уровень.

```python
python
КопироватьРедактировать
# Зададим толеранс по цене для объединения точек в уровень
tol = 0.5 * df['ATR'].mean()  # например, 50% от среднего ATR
support_levels = []    # сюда будут сохраняться цены уровней поддержки
resistance_levels = []  # цены уровней сопротивления

# Сортируем по значению цены (второй элемент кортежа)
local_min_points.sort(key=lambda x: x[1])
local_max_points.sort(key=lambda x: x[1])

# Группируем минимумы
i = 0
while i < len(local_min_points):
    cluster_prices = [local_min_points[i][1]]
    j = i + 1
    # собираем все точки, которые находятся в пределах tol от первой цены в кластере
    while j < len(local_min_points) and abs(local_min_points[j][1] - local_min_points[i][1]) <= tol:
        cluster_prices.append(local_min_points[j][1])
        j += 1
    # вычисляем цену уровня как среднее (можно также взять минимальную или максимальную в кластере)
    level_price = sum(cluster_prices) / len(cluster_prices)
    support_levels.append(level_price)
    # перескакиваем к следующему кластеру
    i = j

# Группируем максимумы (аналогично)
i = 0
while i < len(local_max_points):
    cluster_prices = [local_max_points[i][1]]
    j = i + 1
    while j < len(local_max_points) and abs(local_max_points[j][1] - local_max_points[i][1]) <= tol:
        cluster_prices.append(local_max_points[j][1])
        j += 1
    level_price = sum(cluster_prices) / len(cluster_prices)
    resistance_levels.append(level_price)
    i = j

# Удалим дубли и отсортируем полученные уровни
support_levels = sorted(set(round(x, 6) for x in support_levels))
resistance_levels = sorted(set(round(x, 6) for x in resistance_levels))
print("Уровни поддержки:", support_levels)
print("Уровни сопротивления:", resistance_levels)

```

*Комментарии:*

- Мы округляем уровни до разумного количества знаков (например, 6 знаков после запятой для валютных пар) и убираем дубли через `set`. Округление нужно, потому что могут получиться очень близкие значения, отличающиеся на крошечную долю из-за среднего.
- В результате получаем два списка цен: `support_levels` и `resistance_levels`.
- Каждая цена – это среднее нескольких экстремумов, лежавших близко. Можно было взять и границы кластера (мин и макс в группе) для определения зоны, но для простоты возьмем одну ценовую линию.
- **ATR как люфт:** Здесь `tol` пропорционален ATR (50% от среднего ATR). Чем больше ATR, тем больше разброс значений будет считаться одним уровнем. Эту зависимость можно настраивать: коэффициент 0.5, 1.0, 2.0 и т.д., или даже брать ATR конкретного бара экстремума. Вручную подбор важен – если взять слишком маленький tol, уровней получится очень много (почти каждый экстремум свой уровень); если слишком большой – слипнутся даже далекие уровни.
- В литературе встречаются подходы, где уровни строят как **зоны** шириной в один ATR, вокруг средней цены уровня. Например, нарисовать прямоугольник ±0.5*ATR от средней цены – так видна зона поддержки/сопротивления. Мы же пока оперируем только линиями уровней.
    
    [tradingview.com](https://www.tradingview.com/script/oY7rfzuc-ATR-Based-Support-and-Resistance-Zones-UAlgo/#:~:text=ATR%20Multiplier%3A%20The%20ATR%20multiplier,trading%20preferences%20and%20market%20conditions)
    

### 3. Ранжирование уровней (опционально)

Можно оценить **значимость уровня** по количеству экстремумов в кластере. У нас пока сохранились только усредненные цены, но мы можем легко посчитать длину `cluster_prices` при вычислении уровня – она равна числу точек, образовавших уровень. Эти точки – касания уровня ценой. Логично, что уровень, который price касался 5 раз, сильнее уровня с 2 касаниями

[stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=a%20fixed%20constant%20then%20you,turning%20points%2C%20indicating%20possible%20support%2Fresistance)

.

Для простоты, можно сохранить дополнительно словари: `level_strength = {price: touches}` и, например, затем сортировать уровни по `touches` или просто иметь это число для информации. В нашем примере документации мы не будем глубоко использовать ранжирование, но отметим где нужно:

```python
python
КопироватьРедактировать
# Предположим, внутри цикла, когда вычисляем level_price для support_levels, мы знаем len(cluster_prices)
level_strengths_support = {}
# ...
    level_price = sum(cluster_prices) / len(cluster_prices)
    support_levels.append(level_price)
    level_strengths_support[level_price] = len(cluster_prices)
# ... аналогично для resistance_levels

```

Теперь в `level_strengths_support[level]` будет количество экстремумов, легших в этот уровень. Это можно использовать при выводе (например, подписывать уровень количеством касаний) или при фильтрации (отбросить уровни с 1 касанием, как слабые).

### 4. Результат алгоритмического метода

Алгоритм собрал уровни поддержки и сопротивления. Важно проверить результаты визуально или логически:

- **Число уровней:** Обычно на графике за период бывает несколько ключевых уровней. Если алгоритм вернул слишком много, возможно, tol стоит увеличить или применить больше фильтров.
- **Расположение уровней:** Имеет смысл, чтобы все уровни поддержки были ниже текущей цены, а сопротивления – выше (или уровнями «вокруг» текущей цены). Однако, если данные за длинный период, уровень поддержки в начале периода может сейчас оказаться выше текущей цены, превратившись в сопротивление. Алгоритм в текущем виде этого контекста не учитывает. При необходимости можно фильтровать уровни по актуальности (например, последние N баров).
- **Совпадение уровней:** Иногда уровень из локальных минимумов и уровень из локальных максимумов могут оказаться почти одинаковыми по цене (например, исторический уровень, где раньше был максимум, потом стал минимум). Их можно объединять. Наш алгоритм этого не делал явно (мы отдельно список сделали). Можно объединить оба списка и снова сгруппировать с маленьким tol, чтобы слить очень близкие уровни поддержки/сопротивления.

В целом, чисто алгоритмический подход описан. Он фактически повторяет метод, предлагаемый трейдерами: поискать локальные **turning points** (точки разворота тренда) и соединить их по горизонтали

[stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=Once%20you%20have%20local%20maxima,turning%20points%2C%20indicating%20possible%20support%2Fresistance)

.

Для проверки концепции, приведем небольшой пример на синтетических данных:

```python
python
КопироватьРедактировать
# Пример: синтетический ряд с колебаниями и шумом (данные df_sim)
import numpy as np
import pandas as pd

# Создаем синусоидальный тренд с шумом
x = np.arange(0, 100, 1)
price = 100 + 10*np.sin(x/10) + np.random.normal(0, 1, len(x))
dates = pd.date_range("2023-01-01", periods=len(x), freq='D')
df_sim = pd.DataFrame({
    'close': price,
    'high': price + np.random.normal(0, 0.5, len(x)),
    'low': price - np.random.normal(0, 0.5, len(x))
}, index=dates)

# (Далее применяем поиск экстремумов и уровней на df_sim по аналогии с кодом выше...)

```

Этот код создаёт колеблющийся ряд цен, к которому можно применить алгоритм. Конечно, в реальности данные будут не синусоидальные, но на этом примере можно отладить параметры.

После получения списков `support_levels` и `resistance_levels` переходим к следующему подходу – с использованием машинного обучения, а затем к визуализации.

## Метод поиска уровней с использованием AI/ML

Методы машинного обучения позволяют автоматически **обнаруживать скрытые закономерности** и группы в данных. В контексте уровней поддержек/сопротивлений можно применить:

- **Надзорные (supervised) методы**: когда у нас есть размеченные данные – например, пометки, где на истории были уровни (что требует разметки вручную или алгоритмически). Тогда можно обучить модель предсказывать уровни или классифицировать точки как "уровень/не уровень".
- **Безнадзорные (unsupervised) методы**: когда явных меток нет, но мы можем искать структуры в данных. Например, кластеризация значений цен, чтобы выделить ценовые диапазоны, в которых цена "застревает" или разворачивается.

В рамках данной документации остановимся на двух идеях:

1. **Кластеризация экстремумов с помощью K-Means (Scikit-Learn)** – в принципе, похожа на нашу ручную группировку, но выполненная алгоритмом машинного обучения кластеров.
2. **Пример использования нейронной сети (PyTorch)** – для классификации или регрессии уровней, чтобы показать возможность обучения модели различать точки разворота.

### Кластеризация ценовых точек (K-Means и др.)

Алгоритм **K-Means** пытается разбить набор точек на K кластеров, минимизируя разброс точек внутри кластеров. В нашем случае, если мы подадим в K-Means **набор цен экстремумов**, он разобьет их на K групп по близости ценовых значений. Центры этих кластеров можно рассматривать как **потенциальные уровни**

[alpharithms.com](https://www.alpharithms.com/calculating-support-resistance-in-python-using-k-means-clustering-101517/#:~:text=In%20this%20article%2C%20we%20discuss,end%2C%20we%E2%80%99ll%20cover%20the%20following)

.

Однако следует быть осторожным:

- Нужно задать число кластеров K вручную. Слишком большое K – получим много уровней, слишком маленькое – значимые уровни смешаются.
- K-Means подразумевает, что кластеры приблизительно равносильно определимы (по дисперсии), что может не отражать реальность рынка (где есть несколько сильных уровней и много мелких).
- Есть альтернативы: **Mean-Shift, DBSCAN, Affinity Propagation, Birch** и др., которые не требуют заранее знать количество кластеров и могут автоматически выявлять сколько их на основе расстояния между точками. Например, DBSCAN можно настроить так, что он объединит точки в кластеры при заданном eps (аналог толеранса) и min_samples (минимум точек для образования кластера).
    
    [github.com](https://github.com/Coelodonta/Machine_Learning_Support_Resistance#:~:text=Most%20implementations%20I%27ve%20seen%20use,appropriate%20for%20some%20use%20cases)
    

Тем не менее, для иллюстрации применим K-Means. Возьмем те же списки `local_min_points` и `local_max_points` из алгоритмического метода, составим массив всех цен экстремумов и разобьем на K кластеров.

```python
python
КопироватьРедактировать
from sklearn.cluster import KMeans
import numpy as np

# Соберем все цены экстремумов (можно по отдельности для поддержек/сопротивлений,
# но рассмотрим объединенные для наглядности).
extreme_prices = [price for (_, price) in local_min_points + local_max_points]
extreme_prices = np.array(extreme_prices).reshape(-1, 1)  # превращаем в столбец

# Задаем число кластеров K (например, хотим найти 5 уровней суммарно)
K = 5
kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)
labels = kmeans.fit_predict(extreme_prices)
centers = kmeans.cluster_centers_.flatten()

print("Кластерные центры (уровни):", np.round(centers, 4))

```

*Комментарии:*

- Мы формируем `extreme_prices` как массив shape=(N,1), где N – общее число экстремумов. KMeans в scikit-learn ожидает на вход массив объектов и признаков. У нас признак один – само значение цены. Поэтому точки лежат на числовой оси.
- Параметр `n_init=10` означает, что алгоритм запустится 10 раз с разными начальными условиями и выберет лучший результат кластеризации (так повышается стабильность).
- Результат `kmeans.cluster_centers_` – массив центров кластеров размером K. Мы выводим их как потенциальные **значения уровней**.
- Кроме того, массив `labels` содержит номер кластера для каждого экстремума. Можно использовать его, например, чтобы подсчитать, сколько экстремумов в каждом кластере (что аналогично числу касаний уровня):
    
    ```python
    python
    КопироватьРедактировать
    counts = np.bincount(labels)
    for idx, center in enumerate(centers):
        print(f"Уровень ~{center:.2f} содержит {counts[idx]} точек экстремумов")
    
    ```
    
- Подход с K-Means концептуально схож с нашим алгоритмическим объединением, но менее явно учитывает масштаб (мы задали K, но не сказали ничего про tol). Если K неверно выбран, результат будет неинформативен. Например, если у нас реально 3 значимых уровня, а мы попросили K=10, K-Means создаст 10 центров, многие из которых лягут вплотную друг к другу.

**Пример результата (условный):** допустим, K-Means нашел центры: [1.1002, 1.2005, 1.3040, 1.3998, 1.5001] – это 5 уровней. Возможно, первые два очень близки (1.10 и 1.20), а между 1.3040 и 1.3998 довольно далеко (почти 0.10 разницы), т.е. алгоритм взял кластеров больше, чем нужно и "размазал". Нужно подбирать K или использовать метрики качества кластеризации, например **силуэт** (silhouette score) для разных K, чтобы оценить обоснованность разбиения

[github.com](https://github.com/Coelodonta/Machine_Learning_Support_Resistance#:~:text=In%20addition%20to%20generating%20graphs%2C,is%20evaulated%20using%20silhouette%20scoring)

.

**Альтернативы K-Means:**

- **MeanShift**: не требует заранее K, группирует точки по плотности, но может получиться много мелких кластеров если нет четких плотных групп.
- **DBSCAN**: требует задания максимального расстояния eps и мин. количества точек. По сути, DBSCAN с eps ~ tol из алгоритма и min_samples=2 или 3 очень похож на нашу ручную кластеризацию – он объединит все точки, которые связно находятся на расстояниях < eps. Отличие – DBSCAN также может пометить некоторые точки как "шум" (вне кластеров, если точка-экстремум одиночная далеко от других).
- **AffinityPropagation**: автоматически определяет количество кластеров на основе "преференций". Иногда удобен, но более сложный и медленный на больших данных.
- **Иерархическая кластеризация (Birch, Agglomerative)**: можно строить дерево кластеров, обрывать на определенном расстоянии.

Проекты и исследования отмечают, что K-Means – популярный, но не единственный выбор

[github.com](https://github.com/Coelodonta/Machine_Learning_Support_Resistance#:~:text=Most%20implementations%20I%27ve%20seen%20use,appropriate%20for%20some%20use%20cases)

. В одном примере сравнивались K-Means, MeanShift, Birch и др. для поддержки/сопротивления, и оценивалось качество кластеров с помощью silhouette score

[github.com](https://github.com/Coelodonta/Machine_Learning_Support_Resistance#:~:text=In%20addition%20to%20generating%20graphs%2C,is%20evaulated%20using%20silhouette%20scoring)

.

В итоге, кластеризация дает нам набор ценовых уровней. Она, как и алгоритм, опирается на исторические экстремумы. Отличие в том, что алгоритм с порогом ATR более явно управляется параметрами, а ML-метод (кластеризация) может адаптироваться к распределению данных (например, если экстремумы сами по себе сгруппированы плотнее в одном диапазоне и более разрежены в другом, K-Means/DBSCAN это учтет автоматически, а фиксированный tol – нет).

### Использование нейронных сетей (PyTorch) для поиска уровней

Теперь рассмотрим, как можно задействовать **глубокое обучение**. В этой задаче возможны различные подходы:

- Обучить нейронную сеть **классифицировать точки** графика: например, для каждого бара предсказывать, является ли он частью уровня поддержки, сопротивления или не относится к уровню. Это требует разметки или генерации обучающих меток.
- Обучить модель **предсказывать значения уровней** на основании исторических данных (регрессия). Например, на вход дается последовательность цен, а сеть выдает предполагаемое значение ближайшего уровня сопротивления выше цены и уровня поддержки ниже цены. Такой подход сложнее и потребует специальной архитектуры и обучения.
- Использовать **обучение без учителя**: например, автоэнкодеры для обнаружения аномалий (пики/впадины могут считаться аномалиями), или кластеризация, реализованная через нейронную сеть (например, SOM – self-organizing map – для кластеризации ценовых уровней).

Мы покажем простой пример **классификации локальных экстремумов** с помощью нейронной сети. Возьмем задачу: по ряду цен определить, является ли центральная точка локальным максимумом, минимумом или ничем. Можно упростить до бинарной классификации: "точка является экстремумом (любым) или нет".

Для обучения таких моделей можно **сгенерировать обучающие данные** из исторических цен:

- Признаки: значения цен соседних баров (окна вокруг точки).
- Целевой класс: 1, если центральная точка окна – локальный минимум или максимум; 0, если нет.

Конечно, зная будущее (следующий бар), мы точно знаем статус текущего бара. Но при торговле мы не знаем будущего, поэтому практически модель предсказывает с задержкой. Однако для демонстрации принципа возьмём именно определение экстремума постфактум.

**Пример на PyTorch:** Будем использовать небольшой полносвязный нейрон (Multi-Layer Perceptron) для классификации.

```python
python
КопироватьРедактировать
import torch
import torch.nn as nn
import torch.optim as optim

# Формируем обучающий датасет на основе наших данных df (например, df.close_smooth или df.close)
prices = df['close'].values
X_train = []
y_train = []
window = 3  # берем три точки: предыдущая, текущая, следующая
for i in range(1, len(prices)-1):
    # Признаки: цены [i-1, i, i+1]
    features = [prices[i-1], prices[i], prices[i+1]]
    X_train.append(features)
    # Цель: 1 если i - локальный экстремум, иначе 0
    if (prices[i] > prices[i-1] and prices[i] > prices[i+1]) or (prices[i] < prices[i-1] and prices[i] < prices[i+1]):
        y_train.append(1)
    else:
        y_train.append(0)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # как столбец

# Определяем простую модель
class ExtremumClassifier(nn.Module):
    def __init__(self):
        super(ExtremumClassifier, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(window, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
    def forward(self, x):
        return self.layers(x)

model = ExtremumClassifier()
criterion = nn.BCELoss()  # Binary Cross Entropy для бинарной классификации
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Обучение (например, 100 эпох)
for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    output = model(X_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()

# Оценка на обучающих данных (для простоты)
model.eval()
with torch.no_grad():
    preds = (model(X_train) >= 0.5).int()  # порог 0.5
accuracy = float((preds == y_train.int()).sum()) / len(y_train)
print(f"Accuracy on training data: {accuracy:.2f}")

```

*Комментарии:*

- Мы создали обучающую выборку, где каждый пример – тройка цен (предыдущая, текущая, последующая). Это упрощенный взгляд на локальную форму графика.
- Целевой ярлык `y_train` ставится 1, если текущая цена больше соседних (локальный пик) **или** меньше соседних (локальная впадина). Иначе 0.
- Модель `ExtremumClassifier` – простой двухслойный перцептрон: 3 входа -> 16 скрытых нейронов (ReLU) -> 1 выход (через Sigmoid для получения вероятности класса 1).
- Обучаем 100 эпох с Adam. В реальности, нужно иметь обучающую и тестовую выборку, следить за переобучением и т.д. Здесь лишь демонстрация.
- После обучения вычисляем точность на обучающей выборке. Она должна быть высокой, так как задача определяемая (по сути, модель должна выучить правило "больше соседей = 1" или "меньше соседей = 1").

Если всё сделано правильно, модель может достигнуть точности 100% на этой тренировочной выборке, так как условие довольно детерминировано. На реальных данных с шумом и большим окном эта задача сложнее, но выполнима.

**Практический смысл:** Такая модель может быть полезна, например, для **фильтрации ложных сигналов**. Модифицировав подход, можно пытаться предсказывать экстремум **вперед** – т.е. подав модель в будущее, чтобы она распознала приближающийся разворот (например, на основе паттернов свечей, индикаторов). Это уже требует более сложной архитектуры (например, LSTM или CNN, анализирующей форму графика как изображение).

В нашем случае главное – продемонстрировать использование **PyTorch**. Более продвинутый AI подход мог бы включать:

- Обучение модели, которая **по историческим данным строит уровни напрямую**. Можно представить этот процесс как **кластеризацию, реализованную нейросетью** или как **генерацию линий**. Это сложная задача, потому что уровни – глобальное свойство временного ряда. Возможно, нужно применять **Sequence Models** (LSTM/Transformer) или рассматривать график как изображение и решать задачу детекции объектов (линий) на изображении с помощью **CNN**. В научных работах встречаются такие подходы, но они выходят за рамки данной документации.
- Использование **обучения с подкреплением**: агент, торгующий от уровней, может сам выявлять полезные уровни в процессе обучения. Но опять же, это уже совсем другой уровень сложности.

### Резюме по AI/ML методу

С помощью Scikit-Learn и PyTorch мы показали:

- **Кластеризацию цен** для автоматического выявления уровней (unsupervised).
- **Простейшую нейросетевую модель** для распознавания локальных разворотов (supervised, обучение на разметке от алгоритма).

Оба подхода можно комбинировать с алгоритмическим:

- Например, сначала алгоритм находит экстремумы, затем KMeans кластеризует – это комбинация, которая использует ML (KMeans) вместо ручного tol.
- Или алгоритм выдает метки (как мы сделали для обучения сети), а нейросеть учится их предсказывать – в итоге у нас модель, которая может обобщить информацию и, возможно, находить развороты, даже если форма не идеально 3-точечная.

Следующий этап – **визуализация** найденных уровней на графиках, чтобы убедиться в их правильности и наглядно представить результаты.

## Визуализация уровней на графике

Чтобы интерпретировать полученные уровни, их нужно отобразить на ценовом графике. Мы рассмотрим два способа:

- Статический график с помощью **Matplotlib** (удобно для отчетов, ноутбуков, файлов).
- Интерактивный график с помощью **Plotly** (удобно для веб-интерфейса, Dash).

### Визуализация с помощью Matplotlib

Matplotlib – базовая библиотека для построения графиков в Python. Для финансовых данных есть надстройки (например, `mplfinance`, ранее `mpl_finance`), позволяющие рисовать свечи. Но чтобы не добавлять новых библиотек, можно визуализировать хотя бы линию **цены закрытия** и горизонтальные линии уровней.

**Пример:**

```python
python
КопироватьРедактировать
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
# График цены (например, закрытия)
plt.plot(df.index, df['close'], label='Цена закрытия', color='blue')
# Горизонтальные линии уровней поддержки (зеленые) и сопротивления (красные)
for level in support_levels:
    plt.axhline(y=level, color='green', linestyle='--', linewidth=1)
for level in resistance_levels:
    plt.axhline(y=level, color='red', linestyle='--', linewidth=1)
# Пометим на графике сами точки экстремумов для наглядности
min_x = [pt[0] for pt in local_min_points]
min_y = [pt[1] for pt in local_min_points]
max_x = [pt[0] for pt in local_max_points]
max_y = [pt[1] for pt in local_max_points]
plt.scatter(min_x, min_y, marker='^', color='green', label='Локальные минимумы')
plt.scatter(max_x, max_y, marker='v', color='red', label='Локальные максимумы')

plt.title("График цены с уровнями поддержки/сопротивления")
plt.legend()
plt.xlabel("Дата")
plt.ylabel("Цена")
plt.grid(True)
plt.show()

```

*Что делает этот код:*

- Рисует линию цены закрытия (`plt.plot`).
- Для каждого уровня из списков `support_levels` и `resistance_levels` рисует горизонтальную линию `axhline` с соответствующим цветом (зеленый для поддержки, красный для сопротивления). linestyle `'--'` делает пунктирную линию, чтобы визуально отличать уровни от самой ценовой линии.
- Затем наносит отдельные точки экстремумов: треугольники вверх для максимумов и вниз для минимумов, чтобы мы видели, какие точки формируют уровни.
- Добавляет легенду, сетку и подписи.

На графике мы ожидаем увидеть, что зеленые линии проходят через зеленые треугольники (минимумы) или поблизости, а красные – через вершины красных треугольников. Если несколько треугольников сгруппированы рядом, мы увидим одну линию через них.

**Примечание:** Если уровней получилось очень много, график может быть перегружен линиями. Стоит, возможно, отображать лишь наиболее значимые (например, топ-5 по количеству касаний).

Matplotlib позволяет также рисовать **свечи**. Для полноты, вот как можно отрисовать свечной график с помощью `mplfinance` (библиотеку надо установить отдельно):

```python
python
КопироватьРедактировать
import mplfinance as mpf

# Подготовим DataFrame для mplfinance (он ожидает колонки 'Open','High','Low','Close')
df_plot = df[['open','high','low','close']].copy()
# mplfinance требует индекс типа DatetimeIndex (у нас он уже такой)
levels_lines = [ mpf.make_addplot(pd.Series(level, index=df_plot.index), type='line', color='red')
                 for level in resistance_levels ]
# Здесь мы создаем дополнительные линии на графике для каждого уровня (но это будет много линий)
mpf.plot(df_plot, type='candle', style='charles', addplot=levels_lines)

```

Но проще – использовать Plotly, о чем далее.

### Визуализация с помощью Plotly

**Plotly** – библиотека для построения интерактивных графиков, которые можно просматривать в браузере или интегрировать в Dash-приложения. Для финансовых данных у Plotly есть объекты `go.Candlestick` и удобные методы.

Чтобы построить интерактивный график:

1. Создадим объект Figure и добавим на него свечи (или линию цены).
2. Добавим горизонтальные линии уровней.
3. Отобразим figure (в Jupyter просто `fig.show()`, а в Dash – через компонент Graph).

**Пример с Plotly:**

```python
python
КопироватьРедактировать
import plotly.graph_objects as go

# Создаем фигуру
fig = go.Figure()

# Добавляем свечной график. Используем первые 100 строк df для примера
fig.add_trace(go.Candlestick(x=df.index[:100],
                             open=df['open'][:100],
                             high=df['high'][:100],
                             low=df['low'][:100],
                             close=df['close'][:100],
                             name='OHLC'))

# Добавляем уровни как layout-shapes (горизонтальные линии)
level_annotations = []  # для подписи уровней (например, ценой)
for lvl in support_levels:
    fig.add_hline(y=lvl, line_color="green", line_dash="dash", opacity=0.8)
    level_annotations.append(dict(xref='paper', x=1.0, y=lvl, xanchor='left',
                                  text=f"Support {lvl:.2f}", font_color="green", showarrow=False))
for lvl in resistance_levels:
    fig.add_hline(y=lvl, line_color="red", line_dash="dash", opacity=0.8)
    level_annotations.append(dict(xref='paper', x=1.0, y=lvl, xanchor='left',
                                  text=f"Resistance {lvl:.2f}", font_color="red", showarrow=False))

# Добавляем аннотации к фигуре
fig.update_layout(
    title="Уровни поддержки/сопротивления",
    yaxis_title="Цена",
    xaxis_title="Дата",
    annotations=level_annotations
)

fig.show()

```

*Комментарии:*

- `go.Candlestick` отрисовывает свечи по данным. Здесь для примера берем первые 100 точек `df`, чтобы график был не перегружен. В реальном случае, можно весь df использовать.
- `fig.add_hline` – метод высокого уровня для добавления горизонтальной линии. Мы указываем цвет и стиль (пунктир). `opacity=0.8` делает линии чуть прозрачными.
- Аннотации (`level_annotations`) сформированы, чтобы подписать уровни справа на графике (мы используем `xref='paper', x=1.0` что означает на границе области графика справа). Для каждой линии пишем текст "Support 1234.56" или "Resistance 1234.56". Это опционально, но полезно, если хочется видеть числовое значение уровня на графике.
- `fig.update_layout` применяет заголовок и подписи осей, а также включает созданные аннотации.
- `fig.show()` откроет интерактивный график (если вы в Jupyter, он отобразится прямо в блокноте; если запускаете скрипт, может открыться в браузере).

На интерактивном графике можно наводить курсор, увеличивать области, он позволит детальнее изучить, как цена взаимодействовала с нашими уровнями.

### Интерактивное приложение Dash (кратко)

**Dash** – фреймворк для создания веб-приложений на Python, часто используется для финансовых дашбордов. Объединяет Plotly графики с возможностью добавлять интерактивные контролы (фильтры, кнопки и т.д.).

Чтобы интегрировать график уровней в Dash, необходимо:

1. Установить Dash (`pip install dash`).
2. Создать приложение Dash и описать лейаут с графиком.
3. Передать в график наш Plotly Figure или данные.

Пример минимального приложения:

```python
python
КопироватьРедактировать
import dash
from dash import dcc, html

app = dash.Dash(__name__)
app.layout = html.Div([
    html.H1("Анализ уровней поддержки/сопротивления"),
    dcc.Graph(id='price-chart', figure=fig)  # figure = наш Plotly график из предыдущего кода
])

if __name__ == '__main__':
    app.run_server(debug=True)

```

Запустив этот скрипт, вы получите локально веб-сервер с интерфейсом, где отображается заголовок и интерактивный график. В `dcc.Graph` можно также вместо готового `figure=fig` указать `figure={...}` словарь конфигурации или задать `data` и `layout` отдельно. Но проще передать готовый `fig`.

Dash позволяет добавлять **коллбеки** – функции на Python, которые реагируют на действия пользователя (например, изменение выпадающего списка, диапазона дат, нажатие кнопки) и обновляют график или другие элементы. Это вне сферы данной документации, но отмечу: можно, например, сделать слайдер для выбора периода данных или переключатель "показывать поддержки/сопротивления/обо**.".

## Интерпретация результатов

После отображения уровней важно понимать, что мы получили:

- **Уровни поддержки** (зеленые линии) – исторические ценовые диапазоны, ниже которых цена **редко опускалась** в рассмотренном периоде, или опустившись, отскакивала вверх. Они могут служить ориентирами для покупок (в ожидании отскока) или для постановки стоп-лоссов (если цена уверенно прошла ниже поддержки – сигнал ошибочности лонга).
- **Уровни сопротивления** (красные линии) – ценовые диапазоны, выше которых цена **редко поднималась** или от которых часто отбивалась вниз. Считаются зонами для фиксации прибыли по лонгам или поиска точек входа в шорт, а пробой сопротивления – возможностью на продолжение роста (с превращением пробитого уровня в поддержку).

При анализе найденных уровней нужно учесть:

- **Актуальность:** Алгоритм учитывает весь период данных. Некоторые уровни могут относиться к старым данным и больше не актуальны под конец периода. Можно ограничивать поиск уровней окном (например, последние N дней).
- **Ложные уровни:** Уровень мог сформироваться из двух случайных близких экстремумов, не имеющих фундаментального значения. Обычно, чем больше касаний, тем значимее уровень (мы упоминали ранжирование).
- **Пересечение с ценой:** Если текущая цена далеко от некоторых уровней, они могут быть неинтересны для ближайшей торговли.
- **Volatility regime:** При резком изменении волатильности (ATR) в разные периоды, уровень, значимый в спокойное время, может "пробиваться" в высоковолатильное. Поэтому адаптация порога через ATR, что мы сделали, частично учитывает это – в бурном рынке зоны шире.

**Пример интерпретации:** Предположим, на графике последних 3 месяцев для BTCUSDT мы нашли уровень поддержки ~30000 и сопротивление ~40000. Если текущая цена 35000, то:

- При подходе к 30000 вероятен спрос (поддержка), можно ожидать отскок или по крайней мере замедление падения.
- При росте к 40000 вероятна фиксация прибыли и откат (сопротивление).
- Если цена пробивает выше 40000, этот уровень может стать новой поддержкой (классический принцип "пробитое сопротивление становится поддержкой").
- Мы могли также найти уровни внутри этого диапазона, скажем 33000 и 37000 – промежуточные S/R, которые цена несколько раз тестировала.

Автоматически определенные уровни стоит сверять с известными **новостными событиями и объёмами**. Например, уровень может совпадать с точкой, где ранее был всплеск объема или важная новость – что усиливает его значимость.

## Заключение

Мы разработали и задокументировали алгоритм поиска торговых уровней, начиная с получения данных и заканчивая визуализацией:

- Получили исторические данные с биржи Bybit через API (с примерами кода на Pybit) и альтернативно из CSV.
- Обработали данные с помощью Pandas/NumPy, рассчитали индикатор ATR для оценки волатильности.
- Реализовали алгоритмический подход: определение локальных максимумов/минимумов и объединение их в уровни с учетом допуска по цене (связав этот допуск с ATR для динамичности).
- Рассмотрели метод на основе машинного обучения: кластеризацию ценовых точек (K-Means) и пример нейросетевой модели (PyTorch) для распознавания паттернов экстремумов.
- Построили графики с уровнями в Matplotlib и Plotly, а также обозначили как интегрировать интерактивный график в веб-приложение Dash.
- Обсудили, как интерпретировать и использовать полученные уровни при анализе рынка.

Такой инструмент может помочь трейдерам быстрее обнаруживать важные уровни на графике и принимать решения. Однако всегда рекомендуется сочетать автоматический анализ с экспертным взглядом: проверять, соответствуют ли уровни здравому смыслу, учитывать контекст (тренд, фундаментальные факторы) и управлять рисками при торговле от уровней.

**Ссылки на источники и дополнительное чтение:**

- Понятия поддержки и сопротивления, Investopedia.
    
    [investopedia.com](https://www.investopedia.com/trading/support-and-resistance-basics/#:~:text=Technical%20analysis%20acknowledges%20that%20all,at%20which%20buyers%20back%20off)
    
    [investopedia.com](https://www.investopedia.com/trading/support-and-resistance-basics/#:~:text=,identified%20on%20charts%20using%20trendlines)
    
- Алгоритм выявления уровней через локальные экстремумы и кластеризацию (StackOverflow).
    
    [stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=In%20practical%20terms%20this%20could,that%20the%20ZigZag%20indicator%20uses)
    
    [stackoverflow.com](https://stackoverflow.com/questions/8587047/support-resistance-algorithm-technical-analysis#:~:text=Once%20you%20have%20local%20maxima,turning%20points%2C%20indicating%20possible%20support%2Fresistance)
    
- Использование ATR для формирования зон поддержки/сопротивления.
    
    [tradingview.com](https://www.tradingview.com/script/oY7rfzuc-ATR-Based-Support-and-Resistance-Zones-UAlgo/#:~:text=ATR%20Length%3A%20This%20setting%20allows,broader%20view%20of%20market%20volatility)
    
- Кластеризация уровней с помощью K-Means и других алгоритмов (alpharithms, GitHub project).
    
    [alpharithms.com](https://www.alpharithms.com/calculating-support-resistance-in-python-using-k-means-clustering-101517/#:~:text=In%20this%20article%2C%20we%20discuss,end%2C%20we%E2%80%99ll%20cover%20the%20following)
    
    [github.com](https://github.com/Coelodonta/Machine_Learning_Support_Resistance#:~:text=Most%20implementations%20I%27ve%20seen%20use,appropriate%20for%20some%20use%20cases)
    
- Применение машинного обучения для поиска поддержек/сопротивлений (Incite AI blog).
    
    [inciteai.com](https://www.inciteai.com/articles/analyze-support-and-resistance-of-stocks-with-ai#:~:text=AI%20www,that%20human%20traders%20might%20miss)